{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30096,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Stochastic gradient descent with momentum from scratch","metadata":{}},{"cell_type":"markdown","source":"## Theory\nStochastic Gradient Descent with Momentum is built on Stochastic Gradient Descent. It is fed with a random vector from known data in the training dataset.<br> All the work on calculating the gradient is happening one line at a time. Then we move on to the next random line.<br> It takes a long time, so to speed everything up, we use momentum","metadata":{}},{"cell_type":"markdown","source":"Very interesting, but I found 2 kinds of formulas in different articles: <br>\n1. [An overview of gradient descent optimization algorithms](https://arxiv.org/pdf/1609.04747.pdf): <br>\n$ v_t = \\gamma * v_ {t-1} + \\alpha \\nabla_ \\theta f (\\theta) $ <br>\n$ \\theta = \\theta - v_t $\n2. [An Improved Analysis of Stochastic Gradient Descentwith Momentum](https://arxiv.org/pdf/2007.07989.pdf): <br>\n$ v_t = \\gamma * v_ {t-1} + (1- \\gamma) \\nabla_ \\theta f (\\theta) $ <br>\n$ \\theta = \\theta - \\alpha v_t $ <br>\nWhere: <br> $ \\nabla_ \\theta f (\\theta) $ - partial derivative with respect to $ \\theta $ <br>\n$ \\gamma $ - acceleration weight. $ \\gamma \\in [0,1) $ <br>\n$ \\alpha $ - learning rate (shift step)","metadata":{}},{"cell_type":"markdown","source":"I will implement both and see where the MSE is less after the same number of steps","metadata":{}},{"cell_type":"markdown","source":"We take old class from [multivariant gradient descent](https://www.kaggle.com/konstantinsuspitsyn/multivariate-gradient-descent) and we keep progress_tracker & mse_function","metadata":{}},{"cell_type":"code","source":"class GradientDescents:\n  \n\n  import random\n\n  def progress_tracker(self, step: int, cost_function: float) -> None:\n    '''\n    The function allows you to track online progress\n\n    :param step: current step\n    :param cost_function: the value of the cost function at the moment\n\n    '''\n    from IPython.display import clear_output\n    clear_output(wait=True)\n    print('Step: {}'.format(step))\n    print('Loss: {:.2f}'.format(cost_function))\n\n  def mse_function(self, y_true: list, y_pred: list) -> float:\n    '''\n    Function that calculates MSE\n\n    :param y_true: the y values we know from the actual data\n    :param y_pred: the y values we got at the moment\n\n    :return mse: MSE value\n    '''\n    # Number of values to compare with actual y\n    n = len(y_true)\n    # Starting with 0\n    pre_mse = 0\n    for index, value in enumerate(y_true):\n      pre_mse += (value - y_pred[index])**2\n    mse = pre_mse/n\n    return mse\n\n  def gradient_descent_multi(self, X_true: list, y_true: list, \\\n                              weights: list = None, max_steps: int = 10000, \\\n                              learning_rate: float = 0.003, \\\n                              save_steps: int = 0) -> dict:\n    '''\n    Gradient descent for multiple variables\n\n    :param X_true: actual attributes\n    :param y_true: actual results\n    :param weights: starting weights, if we don't want to start training from random\n    :param learning_rate: learning rate\n    :param max_steps: maximum number of steps at which the algorithm will stop\n    :param save_steps: if 0, only last step will be saved\n                       If not 0, every #save_steps will be saved\n    \n    :return {\n      :return weights: regression weights\n      :return mse: MSE\n      :return steps: # of Steps\n      :return mse_list: list of MSEs if save_steps > 0\n      :return weights_list: list of weigtht lists if save_steps > 0\n    }\n    '''\n\n    # Code for data with only one atribute\n    if (type(X_true[0])==int) or (type(X_true[0])==float):\n      for i, x in enumerate(X_true):\n        X_true[i]=[x,1]\n    elif (type(X_true[0])==list) and (len(X_true[0])==1):\n      for i, x in enumerate(X_true):\n        X_true[i].append(1)\n\n    # Initialize random weights\n    if weights == None:\n      weights = [self.random.random() for f in X_true[0]]\n\n    if save_steps > 0:\n      mse_list = []\n      weights_list = []\n    \n    # MSE of the previous state\n    mse_prev = 0\n    mse = 999\n\n    # Nubmer of experiments we've got\n    n = len(X_true)\n\n    step = 0\n    while (step <= max_steps) and (abs(mse_prev-mse)>1e-5):\n      # Calculate gradients\n      gradients = []\n      for wi, w_value in enumerate(weights):\n        current_gradient=0\n        for yi, y_t_val in enumerate(y_true):\n          current_gradient += -2*(y_t_val - sum([w*x for w,x in \\\n                                                 zip(weights,X_true[yi])]))* X_true[yi][wi]\n        current_gradient = current_gradient/n\n        gradients.append(current_gradient)\n\n      # Change weights\n      for gi, gr_value in enumerate(gradients):\n        weights[gi] = weights[gi] - learning_rate*gr_value\n\n      # Calculate y_pred\n      y_pred = []\n      for X_current in X_true:\n        y_pred.append(sum([w*x for w,x in zip(weights,X_current)]))\n      \n      step +=1\n      mse_prev = mse\n      mse = self.mse_function(y_true, y_pred)\n      self.progress_tracker(step, mse)\n\n      if save_steps > 0:\n        if step % save_steps == 0:\n          mse_list.append(mse)\n          weights_list.append(weights)\n\n    if save_steps > 0:\n      return_dict = {'weights': weights, 'mse':mse, 'steps': step-1, \\\n                      'mse_list': mse_list, 'weights_list': weights_list}\n    else:\n      return_dict = {'weights': weights, 'mse':mse, 'steps': step-1}\n\n    return return_dict","metadata":{"execution":{"iopub.status.busy":"2024-05-20T14:36:00.963900Z","iopub.execute_input":"2024-05-20T14:36:00.964280Z","iopub.status.idle":"2024-05-20T14:36:00.985586Z","shell.execute_reply.started":"2024-05-20T14:36:00.964249Z","shell.execute_reply":"2024-05-20T14:36:00.984392Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"new_grad = GradientDescents()","metadata":{"execution":{"iopub.status.busy":"2024-05-20T14:36:05.651120Z","iopub.execute_input":"2024-05-20T14:36:05.651475Z","iopub.status.idle":"2024-05-20T14:36:05.655195Z","shell.execute_reply.started":"2024-05-20T14:36:05.651444Z","shell.execute_reply":"2024-05-20T14:36:05.654279Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"Realisation\n1. [An overview of gradient descent optimization algorithms](https://arxiv.org/pdf/1609.04747.pdf):<br>\n$v_t = \\gamma * v_{t-1} + \\alpha \\nabla_\\theta f(\\theta)$<br>\n$\\theta = \\theta - v_t$","metadata":{}},{"cell_type":"code","source":"import random","metadata":{"execution":{"iopub.status.busy":"2024-05-20T14:35:55.279790Z","iopub.execute_input":"2024-05-20T14:35:55.280179Z","iopub.status.idle":"2024-05-20T14:35:55.284212Z","shell.execute_reply.started":"2024-05-20T14:35:55.280146Z","shell.execute_reply":"2024-05-20T14:35:55.283268Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# Loading Boston Dataset\nfrom sklearn.datasets import load_boston\nX, y = load_boston(return_X_y=True)\nX_true = []\nfor i in X:\n  x_s_list = [f for f in i]\n  # x_s_list.append(1)\n  X_true.append(x_s_list)\ny_true = [f for f in y]\ndel X, y","metadata":{"execution":{"iopub.status.busy":"2024-05-20T14:35:50.520124Z","iopub.execute_input":"2024-05-20T14:35:50.520635Z","iopub.status.idle":"2024-05-20T14:35:51.289144Z","shell.execute_reply.started":"2024-05-20T14:35:50.520594Z","shell.execute_reply":"2024-05-20T14:35:51.288095Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"learning_rate = 0.0000003\nmax_steps = 50000\nstep = 0\n\n# Momentum\ngamma = 0.5\n\n# For the purity of the choice of the algorithm, I will start the weights from 1\nweights = [1] * len(X_true[0])\n\n# Number of elements in row of X\nn = len(X_true[0])\n\n# Gradients\n# Current gradient\ngradient = []\n# Previous move\nv_t_previous = [0] * len(X_true[0])\n\nall_mses_algo1 = []\n\nwhile step < max_steps:\n  # For experiment we will equal seed with step\n  random.seed(step)\n  # Get random row\n  index = random.randint(0, n-1)\n  # X & y for current step\n  X_current = X_true[index]\n  y_current = y_true[index]\n  gradient = []\n  # Calculate current gradient\n  for x_i in X_current:\n    current_gradient = -2*(y_current - sum([w*x for w,x in \\\n                                          zip(weights,X_current)]))*x_i\n    gradient.append(current_gradient)\n  \n  # Apply momentum for previous step\n  momentum_v_t_previous = [f*gamma for f in v_t_previous]\n  # Applying step for gradient\n  step_gradient = [f*learning_rate for f in gradient]\n  # New delta to move weights\n  v_t = [a+b for a,b in zip(momentum_v_t_previous,step_gradient)]\n  v_t_previous = v_t\n\n  # Move weights\n  for vti, vti_value in enumerate(v_t):\n    weights[vti] = weights[vti] - vti_value\n\n  y_pred = sum([w*x for w,x in zip(weights,X_current)])\n  mse = new_grad.mse_function([y_pred], [y_current])\n\n  step += 1\n\n  new_grad.progress_tracker(step, mse)\n\n  # Check on progress\n  y_pred_algo_1 = []\n  for X_current in X_true:\n    y_pred_algo_1.append(sum([w*x for w,x in zip(weights,X_current)]))\n\n  mse_algo_1 = new_grad.mse_function(y_pred_algo_1, y_true)\n\n\n  all_mses_algo1.append(mse_algo_1)","metadata":{"execution":{"iopub.status.busy":"2024-05-20T14:36:11.922571Z","iopub.execute_input":"2024-05-20T14:36:11.922939Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Step: 3892\nLoss: 120.08\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Try second algorithm<br>\n2. [An Improved Analysis of Stochastic Gradient Descentwith Momentum](https://arxiv.org/pdf/2007.07989.pdf):<br>\n$v_t = \\gamma * v_{t-1} + (1-\\gamma) \\nabla_\\theta f(\\theta)$<br>\n$\\theta = \\theta - \\alpha v_t$<br>","metadata":{}},{"cell_type":"code","source":"learning_rate = 0.0000003\nmax_steps = 50000\nstep = 0\n\n# Momentum\ngamma = 0.9\n\n# For the purity of the choice of the algorithm, I will start the weights from 1\nweights = [1] * len(X_true[0])\n\n# Number of elements in row of X\nn = len(X_true[0])\n\n# Gradients\n# Current gradient\ngradient = []\n# Previous move\nv_t_previous = [0] * len(X_true[0])\nall_mses_algo2 = []\nwhile step < max_steps:\n  # For experiment we will equal seed with step\n  random.seed(step)\n  # Get random row\n  index = random.randint(0, n-1)\n  # X и y for current step\n  X_current = X_true[index]\n  y_current = y_true[index]\n  gradient = []\n  # Calculating current gradient\n  for x_i in X_current:\n    current_gradient = -2*(y_current - sum([w*x for w,x in \\\n                                          zip(weights,X_current)]))*x_i\n    gradient.append(current_gradient)\n  \n  # Apply momentum for previous step\n  momentum_v_t_previous = [f*gamma for f in v_t_previous]\n  # Applying step for gradient\n  step_gradient = [f*(1-gamma) for f in gradient]\n  # New delta to move weights\n  v_t = [a+b for a,b in zip(momentum_v_t_previous,step_gradient)]\n  # В предыдущий сдвиг записываем новый\n  v_t_previous = v_t\n\n  # Make weights move\n  for vti, vti_value in enumerate(v_t):\n    weights[vti] = weights[vti] - vti_value * learning_rate\n\n  y_pred = sum([w*x for w,x in zip(weights,X_current)])\n  mse = new_grad.mse_function([y_pred], [y_current])\n\n  step += 1\n\n  new_grad.progress_tracker(step, mse)\n\n  # Follow the progress\n  y_pred_algo_2 = []\n  for X_current in X_true:\n    y_pred_algo_2.append(sum([w*x for w,x in zip(weights,X_current)]))\n\n  mse_algo_2 = new_grad.mse_function(y_pred_algo_2, y_true)\n\n  all_mses_algo2.append(mse_algo_2)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"steps = [i+1 for i, f in enumerate(all_mses_algo2)]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = plt.figure(figsize=(15, 5))\nax = fig.add_subplot()\nax.plot(steps, all_mses_algo1, color='#ff6361', \\\n        label='First model')\nplt.plot(steps, all_mses_algo2, color='#ffa600', \\\n         label='Second model')\nplt.title('Models Comparison All')\nax.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = plt.figure(figsize=(15, 5))\nax = fig.add_subplot()\nax.plot(steps[:500], all_mses_algo1[:500], color='#ff6361', \\\n        label='First model')\nplt.plot(steps[:500], all_mses_algo2[:500], color='#ffa600', \\\n         label='Second model')\nplt.title('Models Comparison Head')\nax.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = plt.figure(figsize=(15, 5))\nax = fig.add_subplot()\nax.plot(steps[-2000:], all_mses_algo1[-2000:], color='#ff6361', \\\n        label='First model')\nplt.plot(steps[-2000:], all_mses_algo2[-2000:], color='#ffa600', \\\n         label='Second model')\nplt.title('Models Comparison Tail')\nax.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Tests show that the first model is more efficient. We will put it in the class","metadata":{}},{"cell_type":"markdown","source":"# Final Class","metadata":{}},{"cell_type":"code","source":"class GradientDescents:\n  \n  '''\n  Gradient descent from scratch\n  '''\n\n  import random\n\n  def progress_tracker(self, step: int, cost_function: float) -> None:\n    '''\n    The function allows you to track online progress\n\n    :param step: current step\n    :param cost_function: the value of the cost function at the moment\n\n    '''\n    from IPython.display import clear_output\n    clear_output(wait=True)\n    print('Step: {}'.format(step))\n    print('Loss: {:.2f}'.format(cost_function))\n\n  def mse_function(self, y_true: list, y_pred: list) -> float:\n    '''\n    Function that calculates MSE\n\n    :param y_true: the y values we know from the actual data\n    :param y_pred: the y values we got at the moment\n\n    :return mse: MSE value\n    '''\n    # Number of values ​​to compare\n    n = len(y_true)\n    # Starting with 0\n    pre_mse = 0\n    for index, value in enumerate(y_true):\n      pre_mse += (value - y_pred[index])**2\n    mse = pre_mse/n\n    return mse\n\n  def gradient_descent_multi(self, X_true: list, y_true: list, \\\n                              weights: list = None, max_steps: int = 10000, \\\n                              learning_rate: float = 0.003, \\\n                              save_steps: int = 0) -> dict:\n    '''\n    Gradient descent for multiple variables\n\n    :param X_true: actual attributes\n    :param y_true: actual results\n    :param weights: starting weights, if we don't want to start training from random\n    :param learning_rate: learning rate\n    :param max_steps: maximum number of steps at which the algorithm will stop\n    :param save_steps: if 0, only last step will be saved\n                       If not 0, every #save_steps will be saved\n    \n    :return {\n      :return weights: regression weights\n      :return mse: MSE\n      :return steps: # of Steps\n      :return mse_list: list of MSEs if save_steps > 0\n      :return weights_list: list of weigtht lists if save_steps > 0\n    }\n    '''\n\n    # Code for data with one attribute\n    if (type(X_true[0])==int) or (type(X_true[0])==float):\n      for i, x in enumerate(X_true):\n        X_true[i]=[x,1]\n    elif (type(X_true[0])==list) and (len(X_true[0])==1):\n      for i, x in enumerate(X_true):\n        X_true[i].append(1)\n\n    # Initizlize random weights\n    if weights == None:\n      weights = [self.random.random() for f in X_true[0]]\n\n    if save_steps > 0:\n      mse_list = []\n      weights_list = []\n    \n    # previous step MSE\n    mse_prev = 0\n    mse = 999\n\n    # Number of experiments we got     \n    n = len(X_true)\n\n    step = 0\n    while (step <= max_steps) and (abs(mse_prev-mse)>1e-5):\n      # Count gratients\n      gradients = []\n      for wi, w_value in enumerate(weights):\n        current_gradient=0\n        for yi, y_t_val in enumerate(y_true):\n          current_gradient += -2*(y_t_val - sum([w*x for w,x in \\\n                                                 zip(weights,X_true[yi])]))* X_true[yi][wi]\n        current_gradient = current_gradient/n\n        gradients.append(current_gradient)\n\n      # Move weights\n      for gi, gr_value in enumerate(gradients):\n        weights[gi] = weights[gi] - learning_rate*gr_value\n\n      # Calculate y_pred\n      y_pred = []\n      for X_current in X_true:\n        y_pred.append(sum([w*x for w,x in zip(weights,X_current)]))\n      \n      step +=1\n      mse_prev = mse\n      mse = self.mse_function(y_true, y_pred)\n      self.progress_tracker(step, mse)\n\n      if save_steps > 0:\n        if step % save_steps == 0:\n          mse_list.append(mse)\n          weights_list.append(weights)\n\n    if save_steps > 0:\n      return_dict = {'weights': weights, 'mse':mse, 'steps': step-1, \\\n                      'mse_list': mse_list, 'weights_list': weights_list}\n    else:\n      return_dict = {'weights': weights, 'mse':mse, 'steps': step-1}\n\n    return return_dict\n\n  def momentum_gradient_descent(self, X_true: list, y_true: list, \\\n                                weights: list = None, max_steps: int = 10000, \\\n                                learning_rate: float = 0.003, gamma: float = 0.9, \\\n                                save_steps: int = 0) -> dict:\n    '''\n    Gradient descent with acceleration\n\n    : param X_true: actual attributes\n    : param y_true: actual results\n    : param weights: starting weights, if we don't want to start training from random\n    : param learning_rate: learning rate\n    : param max_steps: maximum number of steps at which the algorithm will stop\n    : param save_steps: if 0, only the last step will be saved\n                        if the value is nonzero,\n                        every i-th step will be saved\n\n\n    : return {\n      : return weights: regression weights\n      : return mse: MSE value\n      : return steps: number of steps\n      : return mse_list: MSE value during training if save_steps> 0\n      : return weights_list: weights as we learn if save_steps> 0\n    }\n                      \n    '''\n    # Code for data with one attribute\n    if (type(X_true[0])==int) or (type(X_true[0])==float):\n      for i, x in enumerate(X_true):\n        X_true[i]=[x,1]\n    elif (type(X_true[0])==list) and (len(X_true[0])==1):\n      for i, x in enumerate(X_true):\n        X_true[i].append(1)\n\n\n    if save_steps > 0:\n      mse_list = []\n      weights_list = []\n\n    step = 0\n    mse_prev = 999999999\n\n\n    # Random weights\n    if weights == None:\n      weights = [self.random.random() for f in X_true[0]]\n\n    # Amount of elemets in row X\n    n = len(X_true[0])\n\n    # Gradients\n    # Current gradient\n    gradient = []\n    # Previous move\n    v_t_previous = [0] * len(X_true[0])\n    \n    # TO DO: Change it to tain-validation datasets\n    while step < max_steps:\n\n      # We take a random number to select data\n      index = self.random.randint(0, n-1)\n      # X & y for current step\n      X_current = X_true[index]\n      y_current = y_true[index]\n      gradient = []\n      # Calculate current gradient\n      for x_i in X_current:\n        current_gradient = -2*(y_current - sum([w*x for w,x in \\\n                                              zip(weights,X_current)]))*x_i\n        gradient.append(current_gradient)\n      \n      # Apply momentum\n      momentum_v_t_previous = [f*gamma for f in v_t_previous]\n      # Apply step to a gradient\n      step_gradient = [f*learning_rate for f in gradient]\n      # Get a new move\n      v_t = [a+b for a,b in zip(momentum_v_t_previous,step_gradient)]\n      v_t_previous = v_t\n\n      # Make a move to weights\n      for vti, vti_value in enumerate(v_t):\n        weights[vti] = weights[vti] - vti_value\n\n      y_pred = sum([w*x for w,x in zip(weights,X_current)])\n\n      y_pred_algo = []\n      for X_current in X_true:\n        y_pred_algo.append(sum([w*x for w,x in zip(weights,X_current)]))\n\n      \n      mse = self.mse_function(y_pred_algo, y_true)\n\n      if mse < mse_prev:\n        # Saving only best results\n        final_weights = weights\n\n      mse_prev = mse\n\n      step += 1\n\n      self.progress_tracker(step, mse)\n\n      if save_steps > 0:\n        if step % save_steps == 0:\n          mse_list.append(mse)\n          weights_list.append(weights)\n\n    if save_steps > 0:\n      return_dict = {'weights': final_weights, 'mse':mse, 'steps': step-1, \\\n                      'mse_list': mse_list, 'weights_list': weights_list}\n    else:\n      return_dict = {'weights': final_weights, 'mse':mse, 'steps': step-1}\n\n    return return_dict","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Test on Boston Dataset","metadata":{}},{"cell_type":"code","source":"# Load Boston Dataset\nfrom sklearn.datasets import load_boston\nX, y = load_boston(return_X_y=True)\nX_true = []\nfor i in X:\n  x_s_list = [f for f in i]\n  # x_s_list.append(1)\n  X_true.append(x_s_list)\ny_true = [f for f in y]\ndel X, y","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"msgd = GradientDescents()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gd = msgd.momentum_gradient_descent(X_true, y_true, learning_rate=0.000003, max_steps=1000)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred_algo_1 = []\nfor X_current in X_true:\n  y_pred_algo_1.append(sum([w*x for w,x in zip(gd['weights'],X_current)]))\n\nmse_algo_1 = msgd.mse_function(y_pred_algo_1, y_true)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gd_next = msgd.momentum_gradient_descent(X_true, y_true, learning_rate=0.000003, max_steps=10000,\\\n                                         weights = gd['weights'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred_algo_2 = []\nfor X_current in X_true:\n  y_pred_algo_2.append(sum([w*x for w,x in zip(gd_next['weights'],X_current)]))\n\nmse_algo_2 = msgd.mse_function(y_pred_algo_2, y_true)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mse_algo_1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mse_algo_2","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gd_next = msgd.momentum_gradient_descent(X_true, y_true, learning_rate=0.000003, max_steps=50000,\\\n                                         weights = gd_next['weights'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred_algo_3 = []\nfor X_current in X_true:\n  y_pred_algo_3.append(sum([w*x for w,x in zip(gd_next['weights'],X_current)]))\n\nmse_algo_3 = msgd.mse_function(y_pred_algo_3, y_true)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mse_algo_3","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Of course, it would be more correct to calculate MSE on a test dataset, but we see that the result improves as we train","metadata":{}}]}